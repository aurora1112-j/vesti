收到。处理长文本的最佳方式确实是分块进行。

**截断点说明：**
这第一部分（前四分之一）的截断点我设置在 **「核心功能」结束，即将进入「技术架构」** 的地方。这样刚好涵盖了项目的头部视觉展示、核心理念阐述以及功能概览，是一个逻辑完整的段落。

**排版与合并说明：**

1. 我帮你清理了你文本末尾带入的 Git 合并冲突代码（`<<<<<<< HEAD` 等）。
2. 我保留了我们之前确认的最优 2x2 网格界面布局代码，并将它们放在了 README 的最顶部。
3. **我绝对没有更改你的任何原始文本**，仅仅是为你大段的文字添加了标准的 Markdown 标题（`##`, `###`）和列表加粗格式，以保证在 GitHub 上具有极佳的可读性。

你可以直接复制以下代码替换你 README 的第一部分：

---

```markdown
<div align="center">

  <h1>心迹 Vesti</h1>
  <p>本地优先的AI对话记忆中枢 | Local-First AI Conversation Memory Hub</p>

  <h3>📷 Vesti 界面概览与核心功能演示</h3>

  <table border="0" width="100%" cellspacing="0" cellpadding="10">
    <tr>
      <td width="50%" align="center" valign="top">
        <img src=".github/assets/showcase.png" alt="Vesti Sidebar Right" width="100%" style="border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
        <br>
        <sub><b>沉浸式侧边栏 (右侧)</b><br>在浏览网页时，随时呼出记录灵感与摘要。</sub>
      </td>
      <td width="50%" align="center" valign="top">
        <img src=".github/assets/showcase-2.png" alt="Vesti Chats List Left" width="100%" style="border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
        <br>
        <sub><b>全局对话管理 (左侧)</b><br>集中管理所有平台的 AI 对话历史，一键回溯。</sub>
      </td>
    </tr>
    <tr>
      <td width="50%" align="center" valign="top">
        <img src=".github/assets/summary-generation-1.png" alt="ChatGPT Summary Capture" width="100%" style="border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
        <br>
        <sub>🤖 <b>ChatGPT：即时结构化笔记</b><br>无缝跟随对话流，自动提取要点生成清晰摘要。</sub>
      </td>
      <td width="50%" align="center" valign="top">
        <img src=".github/assets/summary-generation-2.png" alt="Claude Insights Generation" width="100%" style="border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
        <br>
        <sub>🧠 <b>Claude：深度思维路径可视化</b><br>梳理长文脉络，呈现核心问题与思考演进过程。</sub>
      </td>
    </tr>
  </table>

  <br>
  
  https://github.com/user-attachments/assets/9f3f04a6-4abd-4d46-bcb5-b0d12e1b732f

  <b>🎥 视频演示：捕获对话与其他试验功能</b>

  <br><br>

  <img src="https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB" alt="React">
  <img src="https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript">
  <img src="https://img.shields.io/badge/Plasmo-090909?style=for-the-badge&logo=googlechrome&logoColor=white" alt="Plasmo">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="MIT License">

</div>

---

## ⚡ 为什么我们需要心迹

在这个与AI共处的时代，我们的思考方式正在发生深刻的变化。与ChatGPT、Claude等大语言模型的对话已经不再是简单的信息查询，而是成为了思考、创作和决策的核心过程。我们在AI的协助下探索新概念、推演复杂问题、打磨创意方案、澄清模糊直觉。这些对话承载着我们最真实的思维轨迹——那些尚未成型的想法、反复权衡的选择、顿悟的瞬间。

但是，现有的AI平台生态存在着一个根本性的异化。我们创造了数据，却从未真正拥有它。

### 数据封建主义的困境
目前的互联网生态本质上是一种WEB 2.0的数字封建制。AI平台是领主，掌握着数据的所有权和解释权。我们在ChatGPT上探讨的产品架构、在Claude上咨询的技术方案、在Gemini上研究的市场分析——这些思考成果在法律和技术层面都不属于你，而属于OpenAI、Anthropic、Google。

这种所有权的异化带来了一系列问题。首先是数据散落。你的思考碎片被锁在不同平台的围墙花园里，微信的数据不会被百度抓取，ChatGPT的历史记录无法导出到Claude。当你需要回溯某个想法时，你必须记得它发生在哪个平台的哪次对话里，否则就永远找不到了。其次是解释权的丧失。平台通过推荐算法和年度总结定义了"什么是重要的"，但这个标准是为了优化平台的商业目标而设计的，而非服务于你的自我理解。你看到的年度报告告诉你"听了多少首歌""聊了多少次天"，但它无法回答"这些对话在你的思想演进中扮演了什么角色"。

更深层的问题是思维叙事权的缺失。人类通过叙事定义自己，我们需要能够回顾过去、看清思维的轨迹、理解今天的自己如何从昨天的困惑中走来。但当思考过程都发生在与AI的对话中，而这些对话又散落在不同平台、难以整合、缺乏洞察时，我们实际上失去了为自己的思维写传记的能力。我们拥有数据的碎片，却失去了叙事的完整性。

### 注意力主权与意义的关系性
心迹的第一个核心信念是：注意力主权先于数据主权。真正的主权不仅在于"我拥有我的数据"，更在于"我定义什么数据能代表我"。这意味着我们不试图记录用户的所有数字活动——那会导致信号被噪音淹没。相反，我们提供工具让用户主动或智能地选择什么值得被记住。

这个理念源自一个深刻的认识：在信息过载的时代，意义不在于占有更多数据，而在于识别什么是真正重要的。你可能一周内在多个AI平台上进行了五十次对话，但其中真正承载思考深度的可能只有五次。如果系统盲目地给所有对话同等权重，反而会模糊真正的信号。因此，心迹的设计从一开始就围绕"帮助用户定义自己的意义标准"展开。

心迹的第二个核心信念是：意义是关系性的而非本质性的。一个想法的重要性不来自于它本身，而来自于它与其他想法的关联。当你在不同时间、不同平台上反复讨论某个主题时，这些讨论片段在孤立状态下可能只是零散的问答，但当它们被串联起来时，就会显现出一个完整的思考演进轨迹。这就是为什么我们的架构设计从一开始就为跨对话的语义关联预留了空间——我们不仅要记录思考的快照，更要揭示思考的动态过程。

心迹的第三个核心信念是：技术应该增强而非替代人的判断。AI的角色不是告诉用户"你应该关注什么"，而是帮助用户看清"你实际上在关注什么"。系统通过学习用户的策展模式、观察用户的行为偏好，可以逐步承担起识别潜在重要内容的工作，但最终的决定权始终在用户手中。这是一种协作式的智能——AI提供建议，人做出选择，然后AI从选择中继续学习，形成一个正向的反馈循环。

### 从工具到伙伴的演进愿景
心迹的长期目标不是成为一个功能丰富的工具，而是演化为一个真正理解用户的思维伙伴。当你积累了数百上千个对话记录后，这些数据本身就构成了一个关于你的丰富知识图谱。通过向量化和语义分析，系统可以识别出你独特的思维模式、决策偏好、知识盲点。它可以在恰当的时机主动推送相关的历史思考，让你的当前探索能够站在过去经验的肩膀上。它可以揭示那些你自己都没有意识到的认知偏好和思维习惯，帮助你更好地理解自己。

这不是科幻，而是基于现有技术的可实现愿景。关键在于建立正确的数据基础设施和设计哲学。心迹的MVP阶段看似功能简单——只是捕获对话、提供搜索、生成摘要——但我们的每一个技术决策都在为这个长期愿景铺路。本地优先的架构确保了数据主权，服务层隔离的设计为未来引入向量数据库预留了接口，提示词的版本化管理让我们可以持续优化AI的理解能力。我们不是在搭建一个demo，而是在建造一个可以持续生长的系统。

---

## ⭐ 核心功能

心迹当前的能力聚焦于数据捕获和基础组织，这些是构建个性化AI记忆层的必要基础设施。

* **实时捕获**：当你在ChatGPT或Claude页面进行对话时，心迹在后台静默工作，自动提取对话内容并存储到本地IndexedDB。这个过程完全自动化且不可感知，无需你主动标记或导出。捕获的对话包含完整的多轮问答内容、时间戳、平台标识等元数据。每个对话会被分配一个唯一的ID，系统能正确识别并避免重复存储。
* **统一浏览**：在浏览器侧边栏打开心迹，你可以看到按时间排列的所有对话卡片。每张卡片显示对话标题、平台标识、时间戳和消息轮数。将鼠标悬停在卡片上，会展开显示对话的前100字摘要以及快捷操作按钮。点击卡片进入详情视图，查看完整的对话历史。
* **全文检索**：通过搜索框输入关键词，瞬间定位到包含该词的所有对话。搜索不仅匹配标题，也匹配对话的完整内容，让你不再遗漏任何重要的思考片段。你还可以按平台筛选，比如只查看来自ChatGPT的对话。
* **智能摘要**：集成ModelScope API，为单个会话生成结构化摘要。这不是简单的内容压缩，而是基于精心设计的提示词工程，能够揭示对话的思维轨迹、关键转折和核心洞察。摘要包含核心问题、思考演进、关键洞见、悬而未决的话题和可行的下一步建议。生成的摘要会被缓存在本地，下次查看同一对话时无需重新生成。
* **周度思维复盘（Weekly Lite）**：每周结束时，心迹可以生成一份轻量级的思维周报，帮助你回顾这一周与AI的对话主题分布。Weekly Lite版本专注于快速识别你的关注焦点——哪些话题占据了你的思考时间，哪些问题是你反复探讨的，以及这周新涌现的兴趣点。这个功能在对话样本较少时依然能提供有价值的复盘视角，让你不会因为忙碌而失去对自己思维方向的感知。系统会自动评估本周对话样本的充分性，如果对话数量较少或讨论深度不足，会明确标注"Insufficient Data"边界提示，保持分析的诚实性而非过度推测。
* **本地优先**：所有数据存储在你的本地设备中，不上传到任何云端服务器。你拥有完整的数据主权，可以随时导出、备份或删除。即使开发者也无法访问你的对话记录。这不仅是技术选择，更是价值立场——我们相信数据应该服务于人，而非被用来分析和操纵人。

```



## 🧩 技术架构

心迹采用现代化的浏览器扩展技术栈构建，确保性能、稳定性和可扩展性。

**前端框架：** 基于React 18和TypeScript开发，所有组件都有严格的类型定义，消除了运行时错误的可能。UI层采用shadcn/ui组件库，确保视觉一致性和交互流畅性。

**样式系统：** 使用Tailwind CSS实现utility-first的样式架构，配合精心设计的色彩Token和间距系统，营造温暖克制的阅读体验。设计语言遵循Claude MCP Apps的美学标准。

**扩展框架：** 采用Plasmo作为Chrome扩展的开发框架，它提供了热重载、TypeScript支持和更优雅的API抽象，大幅提升了开发效率。

**本地存储：** 使用Dexie.js作为IndexedDB的抽象层，实现了类型安全的数据操作和高效的查询性能。所有对话数据以结构化的形式存储在浏览器本地。

**捕获引擎：** 设计了Parser-Observer-Storage三层架构。Observer层监听DOM变化，Parser层根据平台特性提取对话内容，Storage层负责数据持久化。三层之间通过消息传递解耦，确保了系统的可维护性。

**服务层隔离：** UI组件永远不直接操作数据，所有交互通过统一的服务接口进行。这意味着当我们未来引入向量数据库或云端同步时，只需要重写服务层的内部实现，UI层完全不需要改动。

### 技术栈（Tech Stack）

<table>
  <tr>
    <td width="120px"><strong>Frontend</strong></td>
    <td>React 18, TypeScript, Tailwind CSS, shadcn/ui</td>
  </tr>
  <tr>
    <td width="120px"><strong>Core</strong></td>
    <td>Plasmo Framework, Dexie.js (IndexedDB)</td>
  </tr>
  <tr>
    <td width="120px"><strong>AI</strong></td>
    <td>ModelScope API (Qwen-2.5)</td>
  </tr>
</table>

## 🗂️ 目录结构

```text
.
├── frontend/          # Plasmo 扩展主体
├── documents/         # MVP 文档与说明
├── Frontend_Polish/   # UI 原型与设计资产
├── Backend_Trial/     # 后端试验/草案
├── architecture/      # 架构与研究资料
├── release/           # 交付物/构建输出
└── .github/assets/    # README 图像资源
```

## 🚀 快速开始 (离线安装版)

无需配置编程环境，三步即可体验心迹 Vesti！请先访问以下任一项目主页下载最新版本的安装包 (`.zip` 文件)：

<div align="center">
  <a href="https://vesti-landing-page0211.vercel.app/">
    <img src="https://img.shields.io/badge/官网下载-Vercel-black?style=for-the-badge&logo=vercel&logoColor=white" alt="Vercel 官网下载">
  </a>
  <a href="https://modelscope.cn/studios/aurorasein/Vesti/summary">
    <img src="https://img.shields.io/badge/国内加速-ModelScope-624AFF?style=for-the-badge&logo=modelscope&logoColor=white" alt="ModelScope 下载">
  </a>

  <br><br>

  <a href="https://vesti-landing-page0211.vercel.app/">
    <img src=".github/assets/landing-page-preview.png" alt="Vesti 官网预览" width="80%" style="border-radius: 10px; border: 1px solid #30363d;">
  </a>
  <br><sub>👆 点击预览图访问官网，体验更精美的介绍页面</sub>
</div>

### 🛠️ 安装步骤

#### 1. 解压安装包
下载完成后，将 `Vesti_v1.0.zip` 解压到本地任意位置（例如 `D:\Vesti` 或 `~/Documents/Vesti`）。
> ⚠️ **注意**：请确保解压后的文件夹内能直接看到 `manifest.json` 文件。安装完成后，**请勿删除或移动该文件夹**，否则插件将失效。

#### 2. 开启开发者模式
在 Chrome 浏览器地址栏输入 `chrome://extensions/` 并回车进入扩展管理页。务必开启页面右上角的 **“开发者模式 (Developer mode)”** 开关。

#### 3. 加载插件
点击左上角的 **“加载已解压的扩展程序 (Load unpacked)”** 按钮，选择第 1 步中解压得到的 **文件夹**。

---

### 🎉 安装成功！
此时 Vesti 图标将出现在浏览器右上角的工具栏中（如果没有显示，请点击 🧩 拼图图标将其固定）。

**现在，你可以：**
1. 打开 **DeepSeek**、**ChatGPT** 或 **Claude** 网页版开始对话。
2. 留意侧边栏或点击 Vesti 图标，你会发现你的思维轨迹已被自动捕获并生成了精美的“记忆胶囊”。

## 🚀 快速开始

### 环境要求

在开始之前，请确保你的系统满足以下要求。你需要安装Node.js版本18或更高，以及pnpm包管理器。如果你还没有安装pnpm，可以通过npm全局安装它。你还需要使用Chrome浏览器或其他基于Chromium的浏览器，比如Edge或Brave。

### 安装步骤

首先，从GitHub仓库克隆项目代码到本地。打开终端，执行克隆命令，然后进入项目目录。接下来安装项目依赖，这个过程会下载所有必要的包。安装完成后，执行开发构建命令，Plasmo会自动编译代码并生成可加载的扩展文件。
构建成功后，你会在frontend目录下看到一个build文件夹，其中的chrome-mv3-dev子目录就是可以加载的扩展程序。打开Chrome浏览器，在地址栏输入chrome://extensions/，进入扩展管理页面。确保右上角的开发者模式已开启，然后点击左上角的"加载已解压的扩展程序"按钮，选择刚才提到的chrome-mv3-dev目录。
如果一切顺利，你会在扩展列表中看到心迹Vesti的图标。现在打开ChatGPT或Claude的网页，开始一段对话，你会发现心迹已经在后台默默工作了。点击浏览器右上角的心迹图标，或者通过侧边栏按钮打开心迹面板，你应该能看到刚才的对话已经被捕获并显示在时间轴中。

### ⚙️ 配置 ModelScope API

心迹的摘要功能通过调用 ModelScope (魔搭社区) 的大模型 API 来实现。请按照以下步骤获取密钥并完成配置：

#### 第一步：获取访问令牌 (Access Token)

1. 访问 [ModelScope 官网](https://www.modelscope.cn/) 并注册/登录账号。
2. 点击右上角头像进入「个人中心」，在左侧菜单栏找到并点击 **“访问控制”**。
3. 进入 **“访问令牌”** 页面，复制以 `sdk_` 或 `ms_` 开头的字符串。

<div align="center">
<table>
  <tr>
    <td align="center" width="30%">
      <img src=".github/assets/modelscope-sidebar.jpg" alt="侧边栏菜单 - 访问控制" width="100%">
      <br><sub>① ModelScope 个人中心侧边栏</sub>
    </td>
    <td align="center" width="70%">
      <img src=".github/assets/modelscope-access-token.png" alt="复制访问令牌页面" width="100%">
      <br><sub>② 复制生成的 Access Token</sub>
    </td>
  </tr>
</table>
</div>

#### 第二步：在心迹中完成配置

回到心迹扩展，点击右下角的设置图标进入设置页面，找到 ModelScope 配置区域。

1.  **API Key**: 填入上一步复制的令牌字符串。
2.  **Model ID**: 填入你想使用的模型 ID。
    * *推荐使用 Qwen2.5-Coder 系列（如 `Qwen/Qwen2.5-Coder-32B-Instruct`），该系列在代码理解和响应速度上表现优异。*
3.  **测试连接**: 配置完成后，务必点击 **"Test"** 按钮。如果看到 "连接成功" 提示，说明配置已完成。

<div align="center">
  <img src=".github/assets/vesti-settings-modelscope.png" alt="心迹设置页面 ModelScope 配置示例" width="80%">
  <br><sub>✅ 最终配置效果示例（填入 ID 和 Key 后点击测试）</sub>
</div>

---

#### 附录：如何查找其他模型 ID？

如果你想尝试其他模型，可以在 ModelScope 的模型库中搜索。请确保复制的是形如 `组织名/模型名` （英文）的完整 ID 格式。

<div align="center">
  <img src=".github/assets/modelscope-model-ref.png" alt="ModelScope 模型库界面参考" width="100%">
  <br><sub>参考：在模型库列表中查找模型 ID</sub>
</div>

## 📖 使用指南

### 捕获对话

心迹目前支持ChatGPT和Claude两个平台的对话捕获。当你在这两个网站上进行对话时，扩展会自动监听页面变化并提取对话内容。你不需要做任何主动操作，一切都在后台自动完成。
捕获的对话包含完整的多轮问答内容、时间戳、平台标识等元数据。每个对话会被分配一个唯一的ID，即使你在不同时间访问同一个对话页面，系统也能正确识别并避免重复存储。
需要注意的是，Gemini和DeepSeek虽然在界面上有占位符按钮，但MVP版本暂时不支持这两个平台的后端捕获。这是刻意的设计决策，目的是聚焦资源打磨核心体验。后续版本会逐步添加更多平台支持。

### 浏览与搜索

点击浏览器工具栏上的心迹图标，或者通过侧边栏快捷方式，可以打开心迹面板。默认页面是时间轴视图，显示所有捕获的对话按时间倒序排列。每张对话卡片包含标题、平台标签、时间戳和消息轮数。
将鼠标悬停在卡片上，会展开显示对话的前100字摘要，以及编辑、打开原页面、删除等快捷操作按钮。点击卡片本身会进入详情视图，显示完整的对话历史。
在顶部搜索框中输入关键词，可以实时过滤对话列表。搜索支持标题匹配和内容全文匹配，结果会高亮显示匹配的文字。你还可以点击平台标签进行筛选，比如只查看来自ChatGPT的对话。

### 查看详情

在对话详情页中，消息按时间正序排列，用户消息和AI消息通过不同的背景色区分。长消息支持智能折叠，超过500字符的内容会自动收起，显示"展开全文"按钮。
将鼠标悬停在任意消息上，右上角会出现复制按钮，一键复制消息内容到剪贴板。如果消息中包含代码块，会自动应用等宽字体和语法高亮。
详情页顶部的标题栏包含返回按钮、对话标题、平台标签和消息轮数统计。点击平台标签旁边的跳转图标，可以在新标签页中打开对话的原始网页，方便你继续讨论或查看更多上下文。

### 生成摘要

心迹的摘要功能需要调用 ModelScope 的大语言模型 API。要启用这个功能，你需要先获取 API 密钥。访问 ModelScope 官网并注册账号，点击右上角头像进入「个人中心」，在左侧菜单点击「访问令牌」，复制以 sdk_ 或 ms_ 开头的字符串，这就是你的 API 密钥。
在心迹的设置页面中，找到 ModelScope 配置区域，填入密钥。接下来选择模型 ID，请务必使用 组织名/模型名 的完整格式（如 Qwen/Qwen2.5-Coder-32B-Instruct）。我们强烈推荐使用 Qwen2.5-Coder 系列（32B 或 7B 版本），该系列在逻辑推理、代码理解及响应速度上表现优异，且 API 调用相对稳定。 配置完成后，点击测试按钮验证连接是否成功。如果看到 "连接成功" 的提示，说明配置已完成。
现在你可以在任何对话详情页中点击 "生成摘要" 按钮，系统会调用 ModelScope API 分析对话内容并生成结构化摘要。生成的摘要会被缓存在本地，下次查看同一对话时无需重新生成。

### 数据管理

在设置页面中，你可以查看当前的存储用量。心迹显示已使用的空间大小和浏览器分配的总配额，帮助你评估是否需要清理旧数据。
导出数据功能允许你将所有对话记录打包为JSON格式文件，下载到本地。这个功能主要用于数据备份或迁移到其他设备。导出的JSON文件包含完整的对话内容、元数据和时间戳，可以被其他工具读取和分析。
清空数据按钮会删除所有本地存储的对话记录和缓存的摘要。这是一个危险操作，执行前会弹出二次确认对话框。如果你想重新开始或者测试空白状态的界面，可以使用这个功能。

## 🧭 当前限制与未来规划

### MVP 版本的限制

当前版本是概念验证阶段的产品，有一些明确的功能边界。

**平台支持方面**，只有 ChatGPT 和 Claude 能够实际捕获对话，Gemini 和 DeepSeek 仅保留了 UI 占位符。这不是技术限制，而是资源聚焦的选择，后续会逐步扩展平台覆盖。

**数据同步方面**，所有数据都存储在本地浏览器中，不提供云端备份或多设备同步功能。这意味着如果你在不同的电脑上使用心迹，每台设备的数据是独立的。未来我们会提供可选的云端同步服务，但仍然坚持本地优先的原则。

**AI 功能方面**，目前只实现了基于 ModelScope API 的单会话摘要。更高级的功能，比如跨对话的主题聚类、思维模式分析、主动推送相关历史等，都已经在架构设计中预留了空间，但尚未实现。这些功能需要大量的提示词工程和向量化计算，会在后续版本中逐步推出。

**语言支持方面**，界面和文档目前只有中文版本。虽然核心功能对英文对话同样有效，但部分 UI 文案和设置说明还没有国际化。我们会在用户基数扩大后考虑添加多语言支持。


## 🗺️ 短期路线图 (Roadmap)

在接下来的 **3 到 6 个月**内，我们将专注于从“连接”、“可视化”到“深度智能”的全面进化，计划实现以下核心功能：

### 1. 🔌 全面完善平台支持
致力于打破生态壁垒，我们将扩展后端捕获能力，从当前的架构延伸至更多前沿 AI 工具：
- **新增支持**：集成 **Gemini** 与 **DeepSeek**。
- **探索集成**：计划适配 **Perplexity**、**Poe** 等更多生产力工具。

### 2. 📊 上线思维可视化仪表盘
数据不应只是沉睡在硬盘里的字节。我们将引入 **全局 Dashboard**，让您的智力活动“可见”。
- **Activity Heatmap (活跃度热力图)**：记录您的思考节律与高产时刻。
- **Platform Distribution (平台分布)**：审视您对不同 AI 工具的依赖与偏好。

<p align="center">
  <img src=".github/assets/dashboard-effect.png" alt="Dashboard Effect" width="100%">
  <br>
  <em>预览：每一次对话，都将成为数字大脑成长的可视化足迹</em>
</p>

### 3. 🧠 引入本地向量化技术 (Vectorization)
我们将把所有对话内容转换为语义向量并存储在本地，实现真正的 **语义搜索 (Semantic Search)**。
> **场景举例**：当您搜索 `产品设计` 时，系统不再局限于关键词匹配，而是能智能关联并召回所有讨论 `用户体验`、`交互逻辑`、`功能规划` 的相关对话。

### 4. 🕸️ 跨对话主题聚类
基于向量相似度计算，系统将自动识别您在不同时间、不同平台上反复讨论的核心议题，生成 **主题图谱 (Topic Graph)**。
- **洞察流向**：可视化您的思维如何在不同话题间流动。
- **趋势识别**：自动标记哪些问题是您“持续关注”的，哪些是“新涌现”的灵感。

### 5. 🛡️ 可选的端到端加密云同步
我们坚持 **Local-First (本地优先)** 策略，但也将提供便捷的多端体验。
- **安全承诺**：采用端到端加密 (E2EE) 方案，确保即便是存储在服务器上的数据，我们也无法解密查看。
- **用户自主**：云端同步功能默认关闭，由您全权决定是否开启。

### 长期愿景

从更长远的视角看，心迹的目标是成为 **个性化 AI 时代的记忆基础设施**。当你积累了数千个对话记录后，这些数据本身就构成了一个关于你的丰富知识图谱。通过深度学习技术，系统可以识别出你独特的思维模式、决策偏好、知识盲点。

我们设想未来的心迹能够 **主动推送相关记忆**。当你在思考某个问题时，系统检测到话题，自动从历史对话中提取相关片段形成上下文摘要。你不需要主动搜索，系统在恰当的时机提醒你 "你曾经思考过这个"。

更激进的愿景是 **训练一个完全基于你个人对话数据的语言模型**。这个模型不仅理解你说的话，更理解你为什么这么说、你的潜在意图是什么、你可能会遇到什么困难。它不是通用的 AI，而是真正属于你的数字思维伙伴。

这些愿景的实现需要大量的技术积累和算力投入，但心迹的架构设计已经为这些可能性铺平了道路。MVP 阶段的工程决策不是为了快速搭建 Demo，而是为了建造一个可以持续生长的系统。

## 🤝 贡献指南

心迹是一个开源项目，我们欢迎任何形式的贡献。

如果你在使用过程中发现 bug 或有功能建议，请在 **GitHub Issues** 中提交详细描述。如果你想贡献代码，可以 Fork 仓库并提交 **Pull Request**。

在提交代码前，请确保通过 **TypeScript** 类型检查，并遵循项目的代码风格规范。我们使用 **Prettier** 进行代码格式化，使用 **ESLint** 进行语法检查。所有提交的代码都应该有清晰的注释和类型定义。

如果你想参与产品设计讨论或分享使用心得，欢迎加入我们的社区频道。我们相信最好的产品是与用户共同创造的，你的反馈将直接影响心迹的发展方向。

## 📄 许可证

心迹采用 **MIT 许可证** 开源。你可以自由使用、修改和分发这个项目，只需保留原始的版权声明。我们相信开源精神与数据主权的理念是一致的——软件应该服务于人，而非控制人。

## 🙏 致谢

心迹的诞生离不开开源社区的贡献。我们使用了 **Plasmo**、**Dexie.js**、**Tailwind CSS**、**shadcn/ui** 等优秀的开源项目，感谢这些工具的开发者。

我们也感谢 **Claude Code** 和 **Codex** 在产品设计和代码开发过程中提供的协助，这个项目本身就是人机协作的成果。

最重要的是，感谢每一位使用心迹的用户。你们的对话数据是私密的思维轨迹，选择信任我们的产品意味着很大的信心。**我们承诺始终坚持本地优先的原则，让数据主权牢牢掌握在你们手中。**

<div align="center">
  <img src="https://img.shields.io/badge/心迹%20Vesti-让思维有迹可循-4B0082?style=for-the-badge&logo=sparkles&logoColor=FFD700&labelColor=20232A" alt="心迹 Vesti —— 让思维有迹可循">
</div>
